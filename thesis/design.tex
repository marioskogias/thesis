\chapter{BlkKin Design}\label{ch:design}

In the previous chapters we described the various challenges faced when dealing
with distributed tracing and tools that can be used in order to achieve our
tracing goals. In this chapter we cite the design of the our proposed tracing
infrastructure called BlkKin. The name comes from the amalgamation of
\textit{Block storage} and \textit{Zipkin}, which is one of the used building
blocks and was described thoroughly in Section \ref{sec:zipkin}. BlkKin uses
different open-source technologies and is designed to scale. 

By building BlkKin we wanted to create a tracing infrastructure intended to
cover the tracing needs created in software defined and distributed storage
systems (but of course not restricted to them). After investigating the various
needs that this kind of systems and the people developing and monitoring them
have, we tried to summarize the points that are needed for our tracing
infrastructure. We defined the following prerequisites that should be present in
BlkKin:

\begin{description}

\item[low-overhead tracing] \hfill \\
The traced system should be able to continue working in production scale serving
real workloads in order to locate deficiencies and faults that are not obvious
in debugging or tracing mode.

\item[live-tracing]
BlkKin should be able to send traces at the time the are being produced so that
the developer or the administrator can have an overview of the system at that
specific time.

\item[Dapper tracing semantics]
Tracing logic should be implemented in accordance with the concepts used by
Dapper so that causal relationships and cross-layer architecture are depicted
through end-to-end tracing.

\item[User interface]
BlkKin should provide various endpoints for the end user to collect and analyze
data. One of those should be a graphical user interface that gives a graphical
overview of the system's performance per specific layer.

\end{description}

In the following sections we will step by step examine BlkKin. Specifically, in
Section \ref{sec:rationale} we will describe the rationale behind Blkin, while
Section \ref{sec:components} goes through BlkKin's building blocks and Section
\ref{sec:contribution} analyzises the BlkKin contributions. Finally, in Section
\ref{sec:flow} we illustrate the resulting BlkKin architecture and the flow of a
traced request from the time created in the instrumented infrastructure until it
ends up handled by BlkKin and stored.

\section{Design rational}\label{sec:rationale}

Since distributed systems follow a service-oriented architecture and consist of
different software layers communicating with each other, while running on
different cluster nodes, we have to implement a tracing architecture that
provides distributed tracing on each node, but collects all tracing data to a
central repository. Thus, we can discover the relationships between the
different layers. Otherwise, it would be impossible to find out what actions did
a specific event on a specific node triggered throughout the cluster.

So, imitating the monitoring systems architecture as described in Section
\ref{sec:logging-bkg}, BlkKin consists of the following parts

\begin{description}
\item[tracing agent] \hfill \\
This agent runs on every cluster node. It is responsible for capturing traces
both from user and kernel-space. Is supposed to add minimum overhead to the
instrumented application.

\item[central data collector]
The tracing agents from all the cluster nodes that we are interested about,
should send the aggregated tracing information to a central collecting place so
that they can be correlated. This system includes both the collecting part, a
server that receives the data, and a storage system (database, file system),
where information is finally kept.

\item[User interface]
The aggregated information should be available through a Web user-interface that
depicts the correlation between the systems' distinct software layers. Through
that interface the user should be able to locate the information he wants and
make basic queries. Also, this interface should be able to be used as a
monitoring and alerting tool in case something misoperates.

\end{description}

\section{BlkKin building blocks}\label{sec:components}

After having identified our basic needs for BlkKin and respecting the proverb
\textit{`not to reinvent the wheel'}, we turned to the open-source community to
find projects that met our requirements and we could combine to build BlkKin.

First we came across \textit{Zipkin} (see Section \ref{sec:zipkin} for more).
Zipkin implements the Dapper semantics and provides a mechanism for data
aggregation, data storage and a Web UI. So, Zipkin could undertake the
collecting, storing and presenting tracing information leaving us to handle with
application instrumentation and tracing. Another crucial factor in favor of
using Zipkin was the fact that it makes use of Scribe as a collector server.
This is important because instead of storing tracing information in a database,
we can store them in HDFS and run Map-Reduce jobs on them. An mentioned, tracing
is notorious for the amount of data it produces, so we also need distributed
processing in order to extract more perplexed information. In order to
manipulate that amount of data, Twitter engineers used distrubuted NoSQL
databases and especially Cassandra. However, data from Zipkin that are stored in
Cassandra follow a specific indexing pattern that is created in accordance with
Zipkin's quering needs. This pattern makes hard (or even prevents) to run ad-hoc
custom queries to extract any kind of correlation or information such as average
values. Even Twitter uses Hadoop for these purposes.  So we could use Zipkin for
some visualizing purposes and HDFS for custom data manipulation using the same
collecting mechanism and without the need to change data for the one or the
other purpose. The same data stored in the Zipkin database and used to depict
the causal relationships in the Web UI, can be stored in HDFS and investigated
through Map-Reduce jobs.  

Having covered the data collection and storage and the user-interface part, we
had to create or use a system for the tracing agent. After some some custom
endeavours to create such a system, we concluded that in order to be fast, this
system has to use memory-mapped IO and specifically a ring buffer where a
producer and a consumer exchange data. We created such a mechanism using a
shared memory segment where the instrumented application wrote binary data and
another daemon consumed them. However, we found out that to build such a
mechanism, we had to solve multiple synchronization and concurrency problems and
the situation would become more difficult with multi-threaded applications.
Since BlkKin was designed to cope with production scale environments, we
searched for a tested, open-source technology that covers that need. So, we
decided to use LTTng. An mentioned in Section \ref{sec:user-tracing}, LTTng
enables us to trace both kernel and user-space applications with the same
infrastructure. Since LTTng uses tracepoints inserted in the source code, by
using it we can deploy any tracing logic we want. In our case, we had decided
beforehand to use an annotation-based logic through the Dapper semantics.
So,custom instrumentation was exactly what we needed. Also, LTTng provides live
tracing support, which could make use of for BlkKin's monitoring purposes. So,
instantly another prerequisite was covered. Finally, like Syslog, LTTng
implements different severity levels, namely a tracepoint can be considered
emergency or waring for example. So, this enabled us to create different
tracepoint with different severity levels and in case of live monitoring enable
only the most basic ones, while in case of extensive tracing all tracepoints
should be logged.

Finally, since we design BlkKin to be a distributed tracing infrastructure, we
should take special care about the time skews among the cluster node clocks.
When tracing a single node, a single clock is used. However, distributed
applications create challenges concerning clock synchronizations. If we do not
care about clock synchronization, there is serious possibility to find a request
reply virtually taking place before the request itself, because there is time
skew between the sever and the client. Older approaches (\cite{hp}) used
post-processing techniques in order to adjust the time skews. According to these
techniques, each cluster node collects tracing information based on its local
clock, while a specific cluster node is considered anchor. During the tracing
session, periodically each host sends an echo message to the anchor, and the
anchor replies with the sender's timestamp and the anchor's current timestamp.
The sending host receives the reply and records a time measurement consisting of
three timestamps: send, remote (i.e., anchor), and reply. Relying on the fact
that the network communication time is the same for sending and replying, the
time skew can be computed. After the end of the tracing session these timeskews
are interpolated to create the each host's time-skew-with-the-anchor line
throughout the tracing session. Theses skews are applied to the logged
timestamps before the events end up to the database. These approaches performed
quite well, but their major disadvantages were the post-processing overhead to
calculate the skew, which could be significant in case of long tracing sessions,
and the fact that live-tracing was impossible due to this needed post
processing.

However, when the previous methods were developed, NTPs performance was not
acceptable for distributed tracing. According to \cite{hp} using NTP caused
skews over 1000 $\mu$sec. In 2010 NTP version 4 was developed. According its
RFC\footnote{http://tools.ietf.org/html/rfc5905} NTPv4 includes fundamental
improvements in the mitigation and discipline algorithms that extend the
potential accuracy to the tens of microseconds with modern workstations and fast
LANs. After experimenting with NTPv4 performance and accuracy, we decided that
it was adequate for our tracing needs, namely there was no response happening
before its request, only after a few hours of NTP syncing. One deployment that
we also tried without remarkably better result, was to use a cluster node as NTP
server, since we care only about relative timestamps and not absolute. Thus,
exploiting the fast LAN we waited to have better synchronization results, but we
concluded that even a global NTP server operated well.

\section{BlkKin contribution}\label{sec:contribution}

At that point, after having decided about the different building blocks that
BlkKin would use, we had to find a way to make them cooperate.  In this section
we will explain how we made the above systems communicating with each other in
order to created a unified tracing infrastructure and what we needed to add in
order to complete BlkKin as an end-to-end monitoring tool. This contribution is
returned back to the open-source community and the major part was created during
my participation in the Google Summer of Code 2014 in the LTTng project, under
the supervision of Jeremie Galarneau.

\subsection{BlkKin library}
Since we had decided on the tracing infrastructure, we had to find a way to
trace low-overhead application in accordance with the Dapper semantics. Although
Zipkin provides a variety of instrumentation libraries for languages such as
Java, Scala or Python, there was no such library for C/C++. So we created a
C/C++ library in accordance with the Dapper semantics which provides a useful
API with all the functions that a programmer would need to instrument such an
application. This API is further explained in Section \ref{sec:library}.
Although this library in backend-independent, which means that anybody could
just keep the API and implement a custom tracing infrastructure, we implemented
a backed based on LTTng. So, our library makes use of LTTng \texttt{tracepoint}
in order to log the information we want. Apart from the Dapper ids manipulation
this library is a wrapper around LTTng tracing function that encapsulates the
Dapper semantics, without exposing the LTTng backend to the programmer
instrumenting the application.

In addition, since we wanted to use BlkKin for monitoring purposes as well,
BlkKin library should implement a kind of sampling. Otherwise, the amount of
data created would be huge and the network traffic would be really high.
Although a more elaborate mechanism for sampling could have been created, BlkKin
currently implements only a rate sampling which means only 1/N root spans are
actually initiated. Consequently, this sampling will affect the upper software
layers as well. No root span means no children span as well. Since we are
talking about IO requests, the route of only one out of N read/write requests
will be actually traced.

\subsection{Babeltrace plugins}
As mentioned, Scribe uses Thrift as a communication protocol. So, we needed to
implement a mechanism that would tranform LTTng data and send them to Scribe.
LTTng data are encoded using the CTF format and Babeltrace is responsible for
transforming these data to a human readable format. Consequently, we had to
extend Babeltrace to communicate with Scribe. At first, we tried to implement
this functionality within Babeltrace. So firstly we created a Scribe client
written in C\footnote{https://github.com/marioskogias/scribe-c-client} which is
used in Babeltrace. This version of
Babeltrace\footnote{https://github.com/marioskogias/babeltrace/tree/scribe-client}
was abandoned because Babeltrace internal code architecture was hard to extend.
Instead, we decided to use the Babeltrace Python bindings and deploy this
functionality as a Babeltrace plugin. 

So, using these Python bindings and the \texttt{facebook-scribe} module from
\texttt{pip} we created two different plugins. The first plugin in generic and
sends to Scribe any kind of LTTng data after transforming them into \textit{JSON
format}.  Using this plugin, we can avoid the tedious job of searching for
information within log-files. Instead, we can send our data to Scribe and store
them in HDFS. After that creating simple Map-Reduce jobs that read JSON encoded
data can subtract any information we want. The motivation for this functionality
came from the Facebook's equivalent tracing infrastructure called
Scuba\footnote{https://www.facebook.com/notes/facebook-engineering/under-the-hood-data-diving-with-scuba/10150599692628920}
\cite{scuba}. 

The other plugin was designed for Zipkin use only. Using the Python bindings
reads CTF data and expects them to have specific fields that will enable the
creation of Zipkin messages. This plugin will operate only when the BlkKin
instrumentation library with the LTTng backend is used.

Finally, LTTng live tracing supports only CTF to text transformation and the
above plugins could not be used for live tracing. So we had to extend Babeltrace
live support. Because at the spefic moment of BlkKin's development, Babeltrace
underwent a generic code refactoring, we could only give a temporary solution.
After that refactoring the above plugins would work with live support as well.
Consequently, we offered just an evanescent solution for LTTng live tracing for
Zipkin data. According to this solution, LTTng is obliged to log only specific
tracepoints. After that, Babeltrace sends these data to a Python consumer
communicating over a named Pipe. Implementation details are further explained in
Section \ref{sec:bbl-live}.

\subsection{BlkKin Monitoring UI}
Although Zipkin's UI was adequate for investigating the correlations between the
different storage layers, it didn't cover our needs as an alerting tool. To
cover this need, we designed a simple
Python-django\footnote{https://www.djangoproject.com/} application, what
communicates with the Zipkin database. This application is responsible to gather
particular metrics, mostly average values, such as the average network
communication over the last 10 minutes, and present them accordingly. The
administrator should be able to provide threshold values for these metrics. If
the values are under the thresholds the UI shows them green, but when the
metrics are over the threshold values, they are illustrated red. These threshold
values are the result from elaborate data analysis over a wide series of data in
the HDFS.

\section{BlkKin architecture and data flow}\label{sec:flow}

In this section we present the overall BlkKin architecture and data flow. As we
can see in Figure \ref{fig:blkin-internal.png} we have an application written in
C/C++ we are interested to trace or monitor. After we have identified the
different application layers and decided how to implement the Dapper semantics
for it, we instrument its source code using the BlkKik library. The host where
the application runs, runs the LTTng daemons as well. So, whenever an
instrumentation point is reached, a \texttt{tracepoint} logs the tracing
information to LTTng.  After that, depending on whether we are having a live
tracing session or not, the CTF data will be send either to the \texttt{relayd}
or to local storage. In case of live tracing, our version of Babeltrace will
communicate with the \texttt{relayd}, get the CTF data and send them to the
Python consumer over a named Pipe. Then this consumer will transform them into
Scribe messages and send them to the Scribe server. In case of a non-live
tracing session, the CTF data will end up to the local disk. After the end of
the session, using our Babeltrace plugins, will transform them again into Scribe
messages and send them to Scribe.

\diagram{BlkKin Internal Communication}{blkin-internal.png}

One important thing to mention is that, every communication arrow in Figure
\ref{fig:blkin-internal.png} is a TCP connection. This, in conjunction with the
Scribe characteristics, gives us the ability for a variety of BlkKin deployments
within the instrumented cluster. For example we can have multiple relayds and a
single Scribe server. However, as it is going to be clarified in the Evaluation
(Chapter \ref{ch:evaluation}), the most appropriate and even suggested by
Twitter deployment is the one illustrated in Figure \ref{fig:blkin-deploy.png}.
In this deployment, there is a whole BlkKin stack running on each cluster node,
where the instrumented application runs. This includes a local Scribe daemon as
well. This deployment enables us to take advantage of the Scribe batch messaging
capability. All the CTF messages are send to relayd and Babeltrace over
\texttt{localhost} which is faster and then end up to the local Scribe server.
Scribe is optimized to batch messages or temporarily store them locally if the
next Scribe server is unavailable. This lowers the network traffic and prevents
us from data loss. Also, it enables us from changing the final data destination
(Zipkin or HDFS) simply by changing the configuration file from which each local
Scribe daemon gets configured. 

\diagram{BlkKin Deployment}{blkin-deploy.png}
