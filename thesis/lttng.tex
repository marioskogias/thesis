\chapter{Linux Trace Toolkit - next generation (LTTng)}\label{ch:lttng}

In this chapter we analyze Linux Trace Toolkin - next generation (LTTng), which
was our choice for BlkKin's tracing backend, and we describe its internal
characteristics that led us to using it. Specifically, we give an overall
outline of its architecture and basic notions in Section
\ref{sec:lttng-overview}. Then, we describe the buffering scheme used both for
kernel and user space (Section \ref{sec:buffers}) and we continue by citing
kernel and use space implementation mechanism in Sections
\ref{sec:kernel-tracing}, \ref{sec:user-tracing}. Finally we cite the tracing
format used by LTTng (Section \ref{sec:ctf}) and the mechanism for live tracing
in Section \label{sec:relayd}.

\section{Overview}\label{sec:lttng-overview}

Linux Trace Toolkin - next generation is the successor of Linux Trace Toolkit.
It started as the Mathew Desnoyer's PhD dissertation \cite{desnoyer} in École
Polytechnique de Montréal. Since then, it is maintained by EfficiOS
Inc\footnote{http://www.efficios.com/} and the DORSAL lab in  École
Polytechnique de Montréal.

The LTTng project aims at providing highly efficient tracing tools for Linux.
Its tracers help tracking down performance issues and debugging problems
involving multiple concurrent processes and threads. Tracing across multiple
systems is also possible. This toolchain allows integrated kernel and user-space
tracing from a single user interface. It was initially designed and implemented
to reproduce, under tracing, problems occurring in normal conditions. It uses a
linearly scalable and wait-free RCU (Read-Copy Update) synchronization mechanism
and provides zero-copy data extraction. These mechanisms were implemented in
kernel and then ported to user-space as well.
 
Apart from LTTng's kernel tracer and userspace tracer, viewing and analysis
tools are part of the project. In this thesis, we worked with and extended 
\textit{Babeltrace} \footnote{http://lttng.org/babeltrace}.

Apart from the fact LTTng is a complete toolchan that can be easily installed in
almost any Linux distribution and the integrated kernel and user space tracing
offered, we chose LTTng because of its minimal performance overhead. Since it
was initially designed to `reproduce, under tracing, problems occurring in
normal conditions', LTTng was the ideal tool to use for real-time low-overhead,
block-storage tracing with BlkKin.

In order to understand how LTTng manages to have such a good performance, we
have to go through its internals. But first, we give an overview outline of its
architecture and basic components. According to D. Goulet's Master thesis
(\cite{goulet}), LTTng's architecture can be summarized as shown in Figure
\ref{fig:lttng-arch.png}.  

\diagram{LTTng Architecture}{lttng-arch.png}

The \texttt{lttng} command line interface is a small program used to interact
with the session daemon. Possible interaction are creating sessions, enabling
events, starting tracing and so on. The use of this command line tool is further
explained in Chapter \ref{} about how to use BlkKin.

Tracing sessions are used to isolate users from each other and create coherent
tracing data between all tracing sources (Ex: MariaDB vs Kernel). This
\textit{session daemon} routes user commands to the tracers and keeps an
internal state of the requested actions. The daemon makes sure that this
internal state is in complete synchronization with the tracers, and therefore no
direct communication with the tracers is allowed other than via the session
daemon.  This daemon is self-contained between users. Each user can run its own
session daemon but only one is allowed per user. No communication happens
between daemons. 

\textit{Consumer daemons} extract data from buffers containing recorded data and
write it to disk for later analysis. There are two separate consumer daemons,
one handling user space and the second one the kernel. A single consumer daemon
handles all the user space (and similarly for kernel space) tracing sessions for
a given session daemon. It is the session daemon that initiates the execution of
the user space and kernel consumer daemons and feeds them with tracing commands.

LTTng internals define and make use of the following concepts in order to create
an abstraction layer between the user and the tracers.
 
\begin{description}

\item[Domains] 
are essentially a type of tracer or tracer/feature tuple.  Currently, there are
two domains in lttng-tools. The first one is \texttt{UST} which is the global
user space domain. Channels and events registered in that domain are enabled on
all current and future registered user space applications. The other domain is
\texttt{KERNEL}.  Three more domains are not yet implemented but are good
examples of the tracer/feature concept. They are UST PID for specific PID
tracing, UST EXEC NAME based on application name and UST PID FOLLOW CHILDREN
which is the same as tracing a PID but follows spawned children.

\item[Session]
is an isolated container used to separate tracing sources and users from each
other. It takes advantage of the session feature offered by the tracer.  Each
tracing session has a human readable name (Ex.: myapps) and a directory path
where all trace data is written. It also contains the user UID/GID, in order to
handle permissions on the trace data and also determine who can
interact with it. Credentials are passed through UNIX socket for that purpose.

\item[Event] 
relates to a TRACE EVENT statement in your application code or in the Linux
kernel instrumentation.  Using the command line tool \texttt{lttng}, you can
enable and disable events for a specific tracing session on a per domain basis.
An event is always bound to a channel and associated tracing context.

\item[Channel]
is a pipe between an information producer and consumer. They existed in the
earlier LTTng tracers but were hardcoded and specified by the tracer. In the
new LTTng 2.0 version, channels are now definable by the user and completely
customizable (size of buffers, number of subbuffer, read timer, etc.).  A
channel contains a list of user specified events (e.g. system calls and
scheduling switches) and context information (e.g. process id and priority).
Channels are created on a per domain basis, thus each domain contains a list of
channels that the user creates.  Each event type in a session can belong to a
single channel. For example, if event A is enabled in channel 1, it cannot be
enabled in channel 2. However, event A can be enabled in channel 2 (or channel
1 but not both) of another session.

\end{description}

\section{Buffering scheme}\label{sec:buffers}

In this part we analyze the buffering scheme employed by LTTng for efficient
tracing.

As mentioned, a channel is a pipe between an information producer and consumer.
It serves as a buffer to move data efficiently. It consists of one buffer per
CPU to ensure cache locality and eliminate false-sharing. Each buffer is made of
many sub-buffers where slots are reserved sequentially.  A slot is a sub-buffer
region reserved for exclusive write access by a probe.  This space is reserved
to write either a sub-buffer header or an event header and payload. Figure
\ref{fig:buffers.png} shows space being reserved. On CPU 0, space is reserved in
sub-buffer 0 following event 0. In this buffer, the header and event 0 elements
have been complelety written to the buffer. The grey area represents slots for
which associated commit count increment has been done. Committing a reserved
slot makes it available for reading. On CPU n, a slot is reserved in sub-buffer
0 but is still uncommitted. It is however followed by a committed event. This is
possible due to the non serial nature of event write and commit operations. This
situation happens when execution is interrupted between space reservation and
commit count update and another event must be written by the interrupt handler.
Sub-buffer 1, belonging to CPU 0, shows a fully committed sub-buffer ready for
reading.

\diagram{Channel layout}{buffers.png}

Events written in a reserved slot are made of a header and a variable-sized
payload. The header contains information containing the time stamp associated
with the event and the event type (an integer identifier). The event type
information allows parsing the payload and determining its size. The maximum
slot size is bounded by the sub-buffer size. Both the number of the sub-buffers
and their size can be configured by the \texttt{lttng} command line tool.

In order to synchronize the producer and consumer scheme, LTTng makes use of
atomic operations. The two atomic instructions required are the \texttt{CAS}
(Compare-And-Swap) and a simple atomic increment. Each per-CPU buffer has a
control structure which contains the \textit{write count}, the \textit{read
count}, and an array of \textit{commit counts} and \textit{commit seq counters}.
The counters \textit{commit count} keep track of the amount of data committed in
a sub-buffer using a lightweight increment instruction. The \textit{commit seq}
counters are updated with a concurrency-aware synchronization primitive each
time a sub-buffer is filled. The read count is updated using a standard
SMP-aware \texttt{CAS} operation. This is required because the reader thread can
read sub-buffers from buffers belonging to a remote CPU.

\section{Kernelspace tracing}\label{sec:kernel-tracing}

\section{Userspace tracing}\label{sec:user-tracing}

\section{Common Trace Format (CTF)}\label{sec:ctf}

\section{Live tracing}\label{sec:relayd}
